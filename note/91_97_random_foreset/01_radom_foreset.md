<br>
<br>

# `# Random Forest:`

<br>
<br>


## [pdf_link](https://drive.google.com/file/d/1kXNWMzlbJ2b2T1EDEGVq9LN9rT3QRcN4/view?usp=sharing)


<br>
<br>

# `#Lecture: 91 Introduction to Random Forest`

<br>
<br>


**Random forest,XGboost, Gradient Boosting এমন algorithrm যে গুলোর performance প্রায় সব  ml problem এ accuracy অনেক ভালো থাকে।**

- **Random Forest is very very important.**
- **Applicable both in regression and classification problem.**
- **Random foreset with tune any hyperparameter it gives us more accurate result then any other algorithrm.**

### `We need to know, Decision trees`

- Random forest is a collection of decision trees. Forest means collection of trees that`s why we are saying **Random Forest**. But, why random? We know, Bagging-> (Bootstraped + Aggeration). In, Bootstrapping we randomly select column or row (row sampling, column sampling etc.) here, we select element randomly that's why the **random** word come into the picture . 

<br>

### With Code Demo



<br>
<br>

# `#Lecture: 92 Bias Variance Trade-off in RF`

<br>
<br>


### With Code Demo



<br>
<br>

# `#Lecture: 93 Bagging VS RF`

<br>
<br>


### With Code Demo


<br>
<br>

# `#Lecture: 94 RF Hyperparameter`

<br>
<br>

### With code example


<br>
<br>

# `#Lecture: 95 Hyperparameter tunning - GridsearchCV and RandomSearchCV`

<br>
<br>

### With code example


<br>
<br>

# `#Lecture: 96 OOB(Out Of Bag)`

<br>
<br>

### With code example



<br>
<br>

# `#Lecture: 97 Feature importance with Random forest`

<br>
<br>


### With code example


