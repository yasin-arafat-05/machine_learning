<br>

# `#Logistics Regression:`

<br>


### [Note_link](https://drive.google.com/file/d/1PJ3ZSylgEgWZg2jL9HuGtCox2pnL1_Xn/view?usp=sharing)


- **Lecture:69-70** Intro Of LoR, Perceptron Trick, Coding. 

- **Lecture:71** Problem with Perceptron Trick, Introduction to Sigmoid Function.

- **Lecture:72** Problem with sigmoid function apporach, Introduction to Loss Function apporach(Maximum Likelihood and `Cross-Entropy loss function`)

- **Lecture:73** Gradient Descent and Deravative of Sigmoid Function.

- **Lecture:74** Gradient Descent

- **Lecture:75** Accuracy, Confution matrix or Classification matrix

- **Lecture:76** Precision, Recall, F1 score

- **Lecture:77** Softmax regression with **(Cross-Entrophy loss function)**

- **Lecture:78** Polynomial Feature in LoR

- **Lecture:79** Hyperparameter in LoR





<br>
<br>
<br>
<br>
<br>


## [derivation_of_BCE_LOSS_FUNCTION](https://www.python-unleashed.com/post/derivation-of-the-binary-cross-entropy-loss-gradient)


